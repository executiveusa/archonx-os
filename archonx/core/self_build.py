"""
Self-Build Directive
====================
Every agent carries the self-build directive: "improve yourself."

When an agent completes a task, it introspects on what could be better
and feeds improvements back into the flywheel. This is the mechanism
that makes the system compound over time.

The directive is embedded in every agent's execution loop — it's not
optional, it's constitutional.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import Any

logger = logging.getLogger("archonx.core.self_build")


@dataclass
class SelfBuildReport:
    """Report generated by an agent after every task."""
    agent_id: str
    task_id: str
    execution_quality: float   # 0.0 – 1.0 self-assessment
    improvements_found: list[dict[str, Any]] = field(default_factory=list)
    skills_used: list[str] = field(default_factory=list)
    skills_needed: list[str] = field(default_factory=list)  # skills the agent wishes it had
    bottlenecks: list[str] = field(default_factory=list)
    time_taken_ms: float = 0.0


class SelfBuildDirective:
    """
    The constitutional self-improvement loop for every agent.

    After each task:
    1. Agent self-assesses quality
    2. Identifies improvements (fed to flywheel)
    3. Identifies skill gaps (fed to skill registry for future learning)
    4. Reports bottlenecks (fed to infrastructure monitoring)
    """

    QUALITY_THRESHOLD = 0.8  # Below this triggers automatic improvement search

    def __init__(self) -> None:
        self.reports: list[SelfBuildReport] = []

    def generate_report(
        self,
        agent_id: str,
        task_id: str,
        result: dict[str, Any],
        time_taken_ms: float = 0.0,
    ) -> SelfBuildReport:
        """
        Generate a self-build report from task execution results.

        In production, this would use an LLM to introspect on the result.
        For now, uses heuristic quality assessment.
        """
        quality = self._assess_quality(result)
        improvements = []
        skills_needed = []
        bottlenecks = []

        if quality < self.QUALITY_THRESHOLD:
            improvements.append({
                "description": f"Agent {agent_id} scored {quality:.2f} on task {task_id} — below threshold",
                "priority": "high" if quality < 0.5 else "medium",
                "category": "agent_performance",
                "suggested_action": "Review task execution pattern and add specialized skill",
            })

        if result.get("via") == "placeholder":
            skills_needed.append(result.get("subtask", "unknown"))
            improvements.append({
                "description": f"No tool or skill matched for subtask type '{result.get('subtask', 'unknown')}'",
                "priority": "high",
                "category": "skill_gap",
                "suggested_action": "Create or register a skill for this task type",
            })

        if time_taken_ms > 5000:
            bottlenecks.append(f"Slow execution: {time_taken_ms:.0f}ms")
            improvements.append({
                "description": f"Task {task_id} took {time_taken_ms:.0f}ms — above 5s threshold",
                "priority": "medium",
                "category": "performance",
                "suggested_action": "Profile and optimize or parallelize",
            })

        report = SelfBuildReport(
            agent_id=agent_id,
            task_id=task_id,
            execution_quality=quality,
            improvements_found=improvements,
            skills_used=result.get("skills_used", []),
            skills_needed=skills_needed,
            bottlenecks=bottlenecks,
            time_taken_ms=time_taken_ms,
        )
        self.reports.append(report)
        logger.info(
            "SelfBuild [%s]: quality=%.2f improvements=%d gaps=%d",
            agent_id, quality, len(improvements), len(skills_needed),
        )
        return report

    def _assess_quality(self, result: dict[str, Any]) -> float:
        """Heuristic quality assessment. In production → LLM-based."""
        if result.get("status") == "error":
            return 0.2
        if result.get("via") == "placeholder":
            return 0.5
        if result.get("via") in ("tool", "skill"):
            return 0.9
        return 0.7

    @property
    def average_quality(self) -> float:
        if not self.reports:
            return 0.0
        return sum(r.execution_quality for r in self.reports) / len(self.reports)

    @property
    def total_improvements_found(self) -> int:
        return sum(len(r.improvements_found) for r in self.reports)

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "total_reports": len(self.reports),
            "average_quality": round(self.average_quality, 3),
            "total_improvements": self.total_improvements_found,
            "skill_gaps": list({g for r in self.reports for g in r.skills_needed}),
        }
